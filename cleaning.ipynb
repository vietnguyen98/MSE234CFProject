{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MS&E 234 Project\n",
    "\n",
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from random import sample\n",
    "import os\n",
    "import itertools\n",
    "from collections import defaultdict,Counter\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.DataFrame()\n",
    "path = os.getcwd()\n",
    "for i in range(1, 4+1):\n",
    "    df_raw = pd.concat([df_raw, pd.read_csv(f'{path}/netflix-prize-kaggle-data/combined_data_{i}.txt',\n",
    "        header=None,\n",
    "        names=['CustomerID', 'Rating', 'Date'])])\n",
    "df_raw = df_raw.reset_index(drop = True)\n",
    "df_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Generate Training & Testing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(df, users, train):\n",
    "    df = df[df['CustomerID'].isin(users)]\n",
    "    print(f\"Subset of 10000 users: {df.shape[0]} ratings; {df['CustomerID'].nunique()} users\")\n",
    "\n",
    "    if (train == True):\n",
    "        # assume 50% of the transactions are public\n",
    "        idxs = df[['CustomerID']].reset_index().groupby('CustomerID').agg({'index':lambda x: list(x)}).to_numpy().tolist()\n",
    "        drop_idxs = []\n",
    "        for i in range(len(idxs)):\n",
    "            drop_idxs.append(sample(idxs[i][0], len(idxs[i][0]) // 2))\n",
    "        drop_idxs = list(itertools.chain(*drop_idxs))\n",
    "        df = df.drop(drop_idxs)\n",
    "        print(f'After making 50% private: {df.shape[0]} ratings; {df.CustomerID.nunique()} users')\n",
    "    \n",
    "        # only consider users with at least 100 public transactions\n",
    "        df = df[df['CustomerID'].isin(df['CustomerID'].value_counts()[df['CustomerID'].value_counts() >= 100].index)]\n",
    "        print(f'Filter for users with >= 100 public transactions: {df.shape[0]} ratings; {df.CustomerID.nunique()} users')\n",
    "\n",
    "    # Subset the data to analyze only ratings from July 2005\n",
    "    df = df[df['Date'].apply(lambda x: type(x) == str and bool(re.match(r'2005-07.*', x)))]\n",
    "    print(f'Only July 2005: {df.shape[0]} ratings; {df[\"CustomerID\"].nunique()} users')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract movie ID's from raw data, combine these with df\n",
    "# The data files follow the following format:\n",
    "# Movie ID:\n",
    "# CustomerID, Rating, Date\n",
    "# ...\n",
    "def add_movie_info(df, movie_df):\n",
    "    # get movie id boundaries\n",
    "    movie_rows = np.array(movie_df.index)\n",
    "\n",
    "    # create a column with movie ids to add to df\n",
    "    movie_ids = []\n",
    "    curr_idx = 0\n",
    "    for row in df.itertuples(index = True, name = 'Pandas'):\n",
    "        curr_row = row.Index\n",
    "        while curr_idx < len(movie_rows) - 1:\n",
    "            next_movie_row = movie_rows[curr_idx + 1]\n",
    "            if curr_row > next_movie_row:\n",
    "                curr_idx += 1\n",
    "            else:\n",
    "                break\n",
    "        movie_ids.append(curr_idx + 1) # since index of movie IDs starts at 1\n",
    "    \n",
    "    # add movie ids and days\n",
    "    df['MovieID'] = movie_ids\n",
    "    df['Day'] = pd.DatetimeIndex(df['Date']).day\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get movie ids\n",
    "movie_df = df_raw[df_raw['Rating'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a random sample of users\n",
    "users = sample(df_raw.dropna()['CustomerID'].unique().tolist(), 10000)\n",
    "train = generate_dataset(df_raw, users, train=True)\n",
    "train = add_movie_info(train, movie_df)\n",
    "\n",
    "# get test data with users that have over 100 public transactions\n",
    "valid_users = set(train['CustomerID'].unique())\n",
    "test = generate_dataset(df_raw, users, train=False)\n",
    "test = test[test['CustomerID'].isin(valid_users)]\n",
    "test = add_movie_info(test, movie_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing to see double the number of transactions but the same users\n",
    "print(len(train))\n",
    "print(len(test))\n",
    "print(train['CustomerID'].nunique())\n",
    "print(test['CustomerID'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save files for easy loading\n",
    "train.to_csv(path + '/train.csv')\n",
    "test.to_csv(path + '/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Generate Related Movies' Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: 2 sets x, y\n",
    "# output: for binary vectors x and y: cosine similarity = |x and y| / sqrt(|x||y|)\n",
    "def getCosSim(x, y):\n",
    "    return len(x.intersection(y)) / np.sqrt(len(x) * len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: sparse binary adjList\n",
    "# output: dict of the 50 most similar items and scores, in format: dict[movieID] => ([(items, scores)])\n",
    "def getSimListDaily(adjList):\n",
    "    simList = {}\n",
    "    for key1 in adjList.keys():\n",
    "        cosSims = []\n",
    "        for key2 in adjList.keys():\n",
    "            if key1 == key2:\n",
    "                continue\n",
    "            cosSim = getCosSim(adjList[key1], adjList[key2])\n",
    "            cosSims.append((key2, cosSim))\n",
    "        simList[key1] = sorted(cosSims, key = lambda x: (-x[1], x[0]))[:50]\n",
    "    return simList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: dataframe of customerIDs, Ratings, Day of Month, and MovieIDs\n",
    "# input dataframe should contain both public and private data, as the recc system uses all available information\n",
    "# output: list of 50 related movies and similarity ratings per day\n",
    "# on each consecutive day, more data is used by the recc system\n",
    "# uses cosine similarity on a binary matrix\n",
    "def getSimListMonthly(df):\n",
    "    adjList = defaultdict(set)\n",
    "    \n",
    "    simLists = []\n",
    "    \n",
    "    for day in sorted(df[\"Day\"].unique()):\n",
    "        # add new movies from today to adjList\n",
    "        currDF = df[df[\"Day\"] == day]\n",
    "        for row in currDF.itertuples(index = True, name = 'Pandas'):\n",
    "            adjList[row.MovieID].add(row.CustomerID)\n",
    "        \n",
    "        # compute similarity scores\n",
    "        currSimList = getSimListDaily(adjList)\n",
    "        simLists.append(currSimList)\n",
    "    return simLists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_list = getSimListMonthly(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shows the top 5 movies related to movie id (second value) on the last day of July 2005\n",
    "sim_list[30][30][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# save sim_list for easy usage\n",
    "sim_list_file = path + '/sim_list.pkl'\n",
    "with open(sim_list_file, 'wb') as f:\n",
    "    pickle.dump(sim_list, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Inference Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in files\n",
    "path = os.getcwd()\n",
    "train = pd.read_csv(path + '/train.csv', index_col=0)\n",
    "test = pd.read_csv(path + '/test.csv', index_col=0)\n",
    "\n",
    "sim_list_file = path + '/sim_list.pkl'\n",
    "sim_list = None\n",
    "with open(sim_list_file, 'rb') as f:\n",
    "    sim_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 35224 transactions\n",
      "Test set: 70004 transactions\n"
     ]
    }
   ],
   "source": [
    "print(f'Train set: {len(train)} transactions')\n",
    "print(f'Test set: {len(test)} transactions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of transactions: 9040\n",
      "Number of (customer, date) with <= 5 occurrences: 7336\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# group data into transactions -> (user, day)\n",
    "transaction_counts = test.groupby(['CustomerID', 'Day']).count()\n",
    "print(f'Number of transactions: {len(transaction_counts)}')\n",
    "\n",
    "# remove (customer, date) pairs with more than 5 transactions\n",
    "transaction_counts = transaction_counts[transaction_counts['Rating']  <= 5]\n",
    "print(f'Number of (customer, date) with <= 5 occurrences: {len(transaction_counts)}')\n",
    "transaction_counts = transaction_counts.reset_index()\n",
    "\n",
    "# reset the indices\n",
    "train = train.reset_index()\n",
    "test = test.reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = train['CustomerID'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferences = {}\n",
    "accuracy = {}\n",
    "yields = {}\n",
    "observation_period = 1\n",
    "threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user_idx, user in enumerate(users): \n",
    "    # get transactions such that user has no more than 5 per day\n",
    "    valid_days = transaction_counts[transaction_counts['CustomerID'] == user]['Day'].values\n",
    "    \n",
    "    # check to see if user has valid days\n",
    "    if len(valid_days) == 0:\n",
    "        continue\n",
    "\n",
    "    # get valid, public data for those days\n",
    "    valid_transactions = train[(train['CustomerID'] == user) & (train['Day'].isin(valid_days))]\n",
    "    train_idxs = list(valid_transactions.index)\n",
    "    \n",
    "    # get auxillary information\n",
    "    aux = set()\n",
    "    aux_idxs = list(valid_transactions['MovieID'].unique())\n",
    "    num_aux = len(aux_idxs)\n",
    "\n",
    "    # setup data matrix\n",
    "    delta_matrix = {}\n",
    "    target_items = list(test['MovieID'].unique())\n",
    "    for target_item in target_items:\n",
    "        delta_matrix[target_item] = [()] * num_aux\n",
    "\n",
    "    # setup inference, yeild and accuracy dictionaries\n",
    "    user_inferences = {}\n",
    "    user_yields = {}\n",
    "    user_accuracies = {}\n",
    "\n",
    "    # generate inferences for each day\n",
    "    for day in range(1, 32, observation_period):\n",
    "        user_inferences[day] = []\n",
    "\n",
    "        # check to see if there are aux movies to add\n",
    "        if day in valid_days:\n",
    "            new_movies = list(valid_transactions[valid_transactions['Day'] == day]['MovieID'])\n",
    "            aux.update(new_movies)\n",
    "\n",
    "        # update the delta matrix (target item: [] -> position for each aux item on day)\n",
    "        for movie in aux:\n",
    "            # get related items for each movie\n",
    "            movie_sim_list = sim_list[day-1][movie]\n",
    "            similar_movies = [similar_movie[0] for similar_movie in movie_sim_list]\n",
    "\n",
    "            # get index for accessing delta matrix items\n",
    "            aux_idx = aux_idxs.index(movie)\n",
    "            \n",
    "            # iterate through the related items and update the position in each array\n",
    "            for item in target_items:\n",
    "                curr_pos = 0\n",
    "                prev_pos = 0\n",
    "\n",
    "                # scenario 1: the item is similar to the aux item and has previously been seen before today\n",
    "                if item in similar_movies and len(delta_matrix[item][aux_idx]) != 0:\n",
    "                    prev_pos = delta_matrix[item][aux_idx][0]\n",
    "                    curr_pos = similar_movies.index(item) + 1\n",
    "                    delta_matrix[item][aux_idx] = (curr_pos, prev_pos-curr_pos)\n",
    "                # scenario 2: the item is similar to the aux item but has not been seen before today\n",
    "                elif item in similar_movies and len(delta_matrix[item][aux_idx]) == 0:\n",
    "                    curr_pos = similar_movies.index(item)\n",
    "                    delta_matrix[item][aux_idx] = (curr_pos + 1, curr_pos + 1)\n",
    "                # scenario 3: item is not similar to the aux item\n",
    "                else: \n",
    "                    delta_matrix[item][aux_idx] = ()\n",
    "\n",
    "        # generate scores for each target item\n",
    "        for target in delta_matrix:\n",
    "            scores = [delta for delta in delta_matrix[target] if len(delta) != 0]\n",
    "            pos_scores = [score for score in scores if score[1] > 0]\n",
    "\n",
    "            # if there are items with positive movement across enough aux items, add to inference list\n",
    "            if len(pos_scores) != 0:\n",
    "                score = len(pos_scores)/num_aux\n",
    "                if score >= threshold:\n",
    "                    user_inferences[day].append(target)\n",
    "            \n",
    "\n",
    "        # get ground truth for each day\n",
    "        period_test = test[(test['CustomerID'] == user) & (test['Day'] ==  day) & ~(test['index'].isin(train_idxs))]\n",
    "        test_movies = list(period_test['MovieID'])\n",
    "        \n",
    "        # calculate the yield for each day\n",
    "        num_inferences = len(user_inferences[day])\n",
    "        user_yields[day] = num_inferences \n",
    "\n",
    "        # calculate accuracy for each day\n",
    "        if num_inferences != 0:\n",
    "            correct_inferences = [inference for inference in user_inferences[day] if inference in test_movies]\n",
    "            user_accuracies[day] = len(correct_inferences)/num_inferences\n",
    "    \n",
    "    # save inferences, yields, and accuracy\n",
    "    inferences[user] = user_inferences\n",
    "    yields[user] = user_yields\n",
    "    accuracy[user] = user_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
